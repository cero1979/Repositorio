{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis Topol√≥gico de Datos para Descubrimiento de Contenido en Textos\n",
    "\n",
    "Este cuaderno implementa un pipeline completo de An√°lisis Topol√≥gico de Datos (ATD) para caracterizar textos en espa√±ol seg√∫n su contenido narrativo, descriptivo y otros tipos textuales.\n",
    "\n",
    "## Pipeline:\n",
    "1. Selecci√≥n y preparaci√≥n del texto\n",
    "2. Limpieza y preprocesamiento\n",
    "3. Construcci√≥n de grafo de co-ocurrencia con PMI (Pointwise Mutual Information)\n",
    "4. Construcci√≥n del complejo simplicial\n",
    "5. Filtraci√≥n\n",
    "6. Diagramas de persistencia y c√≥digos de barras\n",
    "7. An√°lisis e interpretaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de Dependencias y Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de paquetes necesarios\n",
    "!pip install numpy pandas matplotlib networkx nltk scikit-learn scipy\n",
    "!pip install ripser persim gudhi\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import re\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para ATD\n",
    "import gudhi\n",
    "from ripser import ripser\n",
    "import persim\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Cargar modelo de espa√±ol\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"‚úì Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Texto de Ejemplo\n",
    "\n",
    "Utilizaremos un texto que combina diferentes tipos de contenido:\n",
    "- **Narrativo**: Cuenta una historia con acciones y eventos temporales\n",
    "- **Descriptivo**: Describe lugares, objetos o personas\n",
    "- **Expositivo**: Explica conceptos o informaci√≥n\n",
    "- **Argumentativo**: Presenta opiniones o razonamientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ejemplo = \"\"\"\n",
    "El viaje comenz√≥ al amanecer. Mar√≠a caminaba por el sendero mientras observaba el paisaje. \n",
    "El bosque era denso y oscuro, con √°rboles altos que bloqueaban la luz del sol. Las hojas \n",
    "formaban un dosel verde que se extend√≠a hasta donde alcanzaba la vista.\n",
    "\n",
    "La fotos√≠ntesis es el proceso mediante el cual las plantas convierten la luz solar en energ√≠a. \n",
    "Este mecanismo biol√≥gico es fundamental para la vida en la Tierra. Las plantas absorben di√≥xido \n",
    "de carbono y liberan ox√≠geno, manteniendo el equilibrio atmosf√©rico.\n",
    "\n",
    "De repente, Mar√≠a escuch√≥ un ruido. Se detuvo y mir√≥ a su alrededor. Un ciervo apareci√≥ \n",
    "entre los √°rboles y la observ√≥ con curiosidad. Ella sonri√≥ y continu√≥ su camino.\n",
    "\n",
    "Es evidente que los bosques desempe√±an un papel crucial en nuestro ecosistema. Debemos \n",
    "protegerlos porque proporcionan h√°bitat para la fauna, purifican el aire y regulan el clima. \n",
    "La conservaci√≥n forestal no es opcional, es una necesidad imperativa para las generaciones \n",
    "futuras.\n",
    "\n",
    "Al llegar al claro, Mar√≠a encontr√≥ un peque√±o lago. El agua era cristalina y reflejaba \n",
    "el cielo azul. Se sent√≥ en una roca y sac√≥ su cuaderno para escribir sobre su experiencia.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texto cargado:\")\n",
    "print(f\"Longitud: {len(texto_ejemplo)} caracteres\")\n",
    "print(f\"N√∫mero de l√≠neas: {len(texto_ejemplo.split('.'))}\")\n",
    "print(\"\\nPrimeras l√≠neas:\")\n",
    "print(texto_ejemplo[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpieza y Preprocesamiento del Texto\n",
    "\n",
    "Pasos:\n",
    "- Tokenizaci√≥n\n",
    "- Eliminaci√≥n de stopwords\n",
    "- Lematizaci√≥n\n",
    "- Filtrado de puntuaci√≥n y caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpia y preprocesa el texto\"\"\"\n",
    "    # Procesar con spaCy\n",
    "    doc = nlp(texto.lower())\n",
    "    \n",
    "    # Filtrar tokens: eliminar stopwords, puntuaci√≥n, espacios\n",
    "    tokens_limpios = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop \n",
    "        and not token.is_punct \n",
    "        and not token.is_space\n",
    "        and len(token.text) > 2\n",
    "        and token.is_alpha\n",
    "    ]\n",
    "    \n",
    "    return tokens_limpios, doc\n",
    "\n",
    "# Aplicar limpieza\n",
    "tokens, doc_procesado = limpiar_texto(texto_ejemplo)\n",
    "\n",
    "print(\"Estad√≠sticas del texto procesado:\")\n",
    "print(f\"Tokens originales: {len([t for t in doc_procesado])}\")\n",
    "print(f\"Tokens despu√©s de limpieza: {len(tokens)}\")\n",
    "print(f\"Vocabulario √∫nico: {len(set(tokens))}\")\n",
    "print(f\"\\nPrimeros 30 tokens limpios:\")\n",
    "print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de frecuencias\n",
    "frecuencias = Counter(tokens)\n",
    "palabras_comunes = frecuencias.most_common(15)\n",
    "\n",
    "print(\"Palabras m√°s frecuentes:\")\n",
    "for palabra, freq in palabras_comunes:\n",
    "    print(f\"  {palabra}: {freq}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "palabras, freqs = zip(*palabras_comunes)\n",
    "ax.bar(palabras, freqs, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Palabras', fontsize=12)\n",
    "ax.set_ylabel('Frecuencia', fontsize=12)\n",
    "ax.set_title('Palabras m√°s frecuentes en el texto', fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construcci√≥n de Grafo de Co-ocurrencia con PMI\n",
    "\n",
    "### Pointwise Mutual Information (PMI)\n",
    "\n",
    "PMI mide la asociaci√≥n entre dos palabras:\n",
    "\n",
    "$$PMI(w_i, w_j) = \\log \\frac{P(w_i, w_j)}{P(w_i) \\cdot P(w_j)}$$\n",
    "\n",
    "Donde:\n",
    "- $P(w_i, w_j)$ es la probabilidad de co-ocurrencia\n",
    "- $P(w_i)$ y $P(w_j)$ son las probabilidades individuales\n",
    "\n",
    "Utilizamos una ventana deslizante para capturar contextos locales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_coocurrencias(tokens, ventana=5):\n",
    "    \"\"\"Calcula matriz de co-ocurrencias con ventana deslizante\"\"\"\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_idx = {palabra: idx for idx, palabra in enumerate(vocab)}\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Matriz de co-ocurrencias\n",
    "    coocurrencias = np.zeros((n_vocab, n_vocab))\n",
    "    \n",
    "    # Ventana deslizante\n",
    "    for i, palabra in enumerate(tokens):\n",
    "        idx_palabra = vocab_idx[palabra]\n",
    "        \n",
    "        # Contexto: ventana antes y despu√©s\n",
    "        inicio = max(0, i - ventana)\n",
    "        fin = min(len(tokens), i + ventana + 1)\n",
    "        \n",
    "        for j in range(inicio, fin):\n",
    "            if i != j:\n",
    "                idx_contexto = vocab_idx[tokens[j]]\n",
    "                coocurrencias[idx_palabra, idx_contexto] += 1\n",
    "    \n",
    "    return coocurrencias, vocab, vocab_idx\n",
    "\n",
    "coocurrencias, vocab, vocab_idx = calcular_coocurrencias(tokens, ventana=5)\n",
    "\n",
    "print(f\"Matriz de co-ocurrencias: {coocurrencias.shape}\")\n",
    "print(f\"Total de co-ocurrencias: {int(coocurrencias.sum())}\")\n",
    "print(f\"Co-ocurrencias no cero: {np.count_nonzero(coocurrencias)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_pmi(coocurrencias, vocab):\n",
    "    \"\"\"Calcula Pointwise Mutual Information\"\"\"\n",
    "    n_vocab = len(vocab)\n",
    "    total = coocurrencias.sum()\n",
    "    \n",
    "    # Probabilidades individuales\n",
    "    prob_palabras = coocurrencias.sum(axis=1) / total\n",
    "    \n",
    "    # Matriz PMI\n",
    "    pmi = np.zeros((n_vocab, n_vocab))\n",
    "    \n",
    "    for i in range(n_vocab):\n",
    "        for j in range(n_vocab):\n",
    "            if coocurrencias[i, j] > 0:\n",
    "                prob_conjunta = coocurrencias[i, j] / total\n",
    "                prob_independiente = prob_palabras[i] * prob_palabras[j]\n",
    "                \n",
    "                if prob_independiente > 0:\n",
    "                    pmi[i, j] = np.log(prob_conjunta / prob_independiente)\n",
    "    \n",
    "    # PMI positivo (PPMI) - valores negativos a 0\n",
    "    ppmi = np.maximum(pmi, 0)\n",
    "    \n",
    "    return pmi, ppmi\n",
    "\n",
    "pmi, ppmi = calcular_pmi(coocurrencias, vocab)\n",
    "\n",
    "print(\"Estad√≠sticas de PMI:\")\n",
    "print(f\"  PMI m√≠nimo: {pmi[pmi != 0].min():.3f}\")\n",
    "print(f\"  PMI m√°ximo: {pmi.max():.3f}\")\n",
    "print(f\"  PMI promedio: {pmi[pmi != 0].mean():.3f}\")\n",
    "print(f\"\\nEstad√≠sticas de PPMI:\")\n",
    "print(f\"  PPMI m√°ximo: {ppmi.max():.3f}\")\n",
    "print(f\"  PPMI promedio: {ppmi[ppmi != 0].mean():.3f}\")\n",
    "print(f\"  Conexiones positivas: {np.count_nonzero(ppmi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear grafo de co-ocurrencia\n",
    "def crear_grafo_coocurrencia(ppmi, vocab, umbral=0.5):\n",
    "    \"\"\"Crea grafo de co-ocurrencia usando PPMI\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Agregar nodos\n",
    "    for palabra in vocab:\n",
    "        G.add_node(palabra)\n",
    "    \n",
    "    # Agregar aristas con peso PPMI\n",
    "    n_vocab = len(vocab)\n",
    "    for i in range(n_vocab):\n",
    "        for j in range(i+1, n_vocab):\n",
    "            if ppmi[i, j] > umbral:\n",
    "                G.add_edge(vocab[i], vocab[j], weight=ppmi[i, j])\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = crear_grafo_coocurrencia(ppmi, vocab, umbral=0.3)\n",
    "\n",
    "print(\"Grafo de co-ocurrencia:\")\n",
    "print(f\"  Nodos: {G.number_of_nodes()}\")\n",
    "print(f\"  Aristas: {G.number_of_edges()}\")\n",
    "print(f\"  Densidad: {nx.density(G):.4f}\")\n",
    "print(f\"  Componentes conexas: {nx.number_connected_components(G)}\")\n",
    "\n",
    "# Visualizar grafo\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Grado de nodos para tama√±o\n",
    "node_sizes = [300 + 100 * G.degree(node) for node in G.nodes()]\n",
    "\n",
    "# Pesos de aristas\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "\n",
    "# Dibujar\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.2, width=weights, edge_color='gray', ax=ax)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', \n",
    "                       alpha=0.7, ax=ax)\n",
    "nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold', ax=ax)\n",
    "\n",
    "ax.set_title('Grafo de Co-ocurrencia (PMI)', fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcci√≥n del Complejo Simplicial\n",
    "\n",
    "Construimos un **complejo de Vietoris-Rips** a partir del grafo de co-ocurrencia.\n",
    "\n",
    "Un complejo simplicial captura relaciones de orden superior:\n",
    "- 0-simplices: Palabras individuales (nodos)\n",
    "- 1-simplices: Pares de palabras (aristas)\n",
    "- 2-simplices: Tri√°ngulos de palabras\n",
    "- k-simplices: Grupos de k+1 palabras relacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir grafo a matriz de distancias\n",
    "def grafo_a_matriz_distancia(G, vocab):\n",
    "    \"\"\"Convierte grafo con pesos PPMI a matriz de distancias\"\"\"\n",
    "    n = len(vocab)\n",
    "    distancias = np.full((n, n), np.inf)\n",
    "    \n",
    "    # Diagonal = 0\n",
    "    np.fill_diagonal(distancias, 0)\n",
    "    \n",
    "    # Convertir PPMI a distancia: dist = 1 / (1 + PPMI)\n",
    "    vocab_idx = {palabra: idx for idx, palabra in enumerate(vocab)}\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        i, j = vocab_idx[u], vocab_idx[v]\n",
    "        peso = data['weight']\n",
    "        dist = 1.0 / (1.0 + peso)  # Inversamente proporcional a PMI\n",
    "        distancias[i, j] = dist\n",
    "        distancias[j, i] = dist\n",
    "    \n",
    "    return distancias\n",
    "\n",
    "matriz_distancias = grafo_a_matriz_distancia(G, vocab)\n",
    "\n",
    "print(\"Matriz de distancias:\")\n",
    "print(f\"  Forma: {matriz_distancias.shape}\")\n",
    "print(f\"  Distancia m√≠nima (no-diagonal): {matriz_distancias[matriz_distancias > 0].min():.4f}\")\n",
    "print(f\"  Distancia m√°xima finita: {matriz_distancias[matriz_distancias < np.inf].max():.4f}\")\n",
    "print(f\"  Conexiones finitas: {np.sum(np.isfinite(matriz_distancias)) - len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir complejo de Rips con GUDHI\n",
    "def construir_complejo_rips(matriz_distancias, max_dimension=2, max_edge_length=2.0):\n",
    "    \"\"\"Construye complejo de Vietoris-Rips usando GUDHI\"\"\"\n",
    "    # Reemplazar infinitos con valor grande\n",
    "    matriz_finita = matriz_distancias.copy()\n",
    "    matriz_finita[np.isinf(matriz_finita)] = max_edge_length * 2\n",
    "    \n",
    "    # Crear complejo de Rips\n",
    "    rips_complex = gudhi.RipsComplex(distance_matrix=matriz_finita, \n",
    "                                      max_edge_length=max_edge_length)\n",
    "    \n",
    "    # Crear √°rbol simplex\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=max_dimension)\n",
    "    \n",
    "    return simplex_tree\n",
    "\n",
    "simplex_tree = construir_complejo_rips(matriz_distancias, max_dimension=2, max_edge_length=1.5)\n",
    "\n",
    "print(\"Complejo Simplicial (Vietoris-Rips):\")\n",
    "print(f\"  N√∫mero de simplices: {simplex_tree.num_simplices()}\")\n",
    "print(f\"  N√∫mero de v√©rtices: {simplex_tree.num_vertices()}\")\n",
    "print(f\"  Dimensi√≥n: {simplex_tree.dimension()}\")\n",
    "\n",
    "# Contar simplices por dimensi√≥n\n",
    "print(\"\\nSimplices por dimensi√≥n:\")\n",
    "for dim in range(simplex_tree.dimension() + 1):\n",
    "    count = sum(1 for simplex in simplex_tree.get_skeleton(dim) if len(simplex[0]) == dim + 1)\n",
    "    print(f\"  {dim}-simplices: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filtraci√≥n y Homolog√≠a Persistente\n",
    "\n",
    "La **filtraci√≥n** construye una secuencia de complejos simpliciales crecientes.\n",
    "La **homolog√≠a persistente** identifica caracter√≠sticas topol√≥gicas que persisten a trav√©s de m√∫ltiples escalas:\n",
    "\n",
    "- **H‚ÇÄ**: Componentes conexas (clusters de palabras)\n",
    "- **H‚ÇÅ**: Ciclos (relaciones c√≠clicas entre conceptos)\n",
    "- **H‚ÇÇ**: Cavidades (estructuras de orden superior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular homolog√≠a persistente con GUDHI\n",
    "def calcular_persistencia_gudhi(simplex_tree):\n",
    "    \"\"\"Calcula homolog√≠a persistente usando GUDHI\"\"\"\n",
    "    # Calcular persistencia\n",
    "    persistence = simplex_tree.persistence()\n",
    "    \n",
    "    # Organizar por dimensi√≥n\n",
    "    persistence_por_dim = {}\n",
    "    for dim, (birth, death) in persistence:\n",
    "        if dim not in persistence_por_dim:\n",
    "            persistence_por_dim[dim] = []\n",
    "        persistence_por_dim[dim].append((birth, death))\n",
    "    \n",
    "    return persistence, persistence_por_dim\n",
    "\n",
    "persistence, persistence_por_dim = calcular_persistencia_gudhi(simplex_tree)\n",
    "\n",
    "print(\"Homolog√≠a Persistente:\")\n",
    "print(f\"  Total de caracter√≠sticas: {len(persistence)}\")\n",
    "print(\"\\nCaracter√≠sticas por dimensi√≥n:\")\n",
    "for dim in sorted(persistence_por_dim.keys()):\n",
    "    print(f\"  H{dim}: {len(persistence_por_dim[dim])} caracter√≠sticas\")\n",
    "    \n",
    "    # Calcular persistencia (longevidad)\n",
    "    persistencias = []\n",
    "    for birth, death in persistence_por_dim[dim]:\n",
    "        if death != float('inf'):\n",
    "            persistencias.append(death - birth)\n",
    "    \n",
    "    if persistencias:\n",
    "        print(f\"     Persistencia promedio: {np.mean(persistencias):.4f}\")\n",
    "        print(f\"     Persistencia m√°xima: {np.max(persistencias):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Diagramas de Persistencia y C√≥digos de Barras\n",
    "\n",
    "Visualizamos la homolog√≠a persistente de dos formas:\n",
    "\n",
    "1. **Diagrama de Persistencia**: Puntos (nacimiento, muerte) de caracter√≠sticas\n",
    "2. **C√≥digo de Barras**: Intervalos de existencia de cada caracter√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagrama de persistencia\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Panel 1: Diagrama de persistencia\n",
    "ax = axes[0]\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "\n",
    "for dim in sorted(persistence_por_dim.keys()):\n",
    "    births = []\n",
    "    deaths = []\n",
    "    \n",
    "    for birth, death in persistence_por_dim[dim]:\n",
    "        births.append(birth)\n",
    "        if death == float('inf'):\n",
    "            deaths.append(matriz_distancias[matriz_distancias < np.inf].max())\n",
    "        else:\n",
    "            deaths.append(death)\n",
    "    \n",
    "    ax.scatter(births, deaths, c=colors[dim % len(colors)], \n",
    "               label=f'H{dim}', s=80, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "\n",
    "# L√≠nea diagonal\n",
    "lims = [0, max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.3, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Nacimiento (Birth)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Muerte (Death)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Diagrama de Persistencia', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Informaci√≥n de persistencia\n",
    "ax = axes[1]\n",
    "ax.axis('off')\n",
    "\n",
    "info_text = \"INTERPRETACI√ìN DEL DIAGRAMA\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "\n",
    "for dim in sorted(persistence_por_dim.keys()):\n",
    "    info_text += f\"H{dim} ({len(persistence_por_dim[dim])} caracter√≠sticas):\\n\"\n",
    "    \n",
    "    if dim == 0:\n",
    "        info_text += \"  Componentes conexas (clusters)\\n\"\n",
    "        info_text += \"  Agrupa palabras sem√°nticamente relacionadas\\n\"\n",
    "    elif dim == 1:\n",
    "        info_text += \"  Ciclos 1-dimensionales\\n\"\n",
    "        info_text += \"  Relaciones c√≠clicas entre conceptos\\n\"\n",
    "    elif dim == 2:\n",
    "        info_text += \"  Cavidades 2-dimensionales\\n\"\n",
    "        info_text += \"  Estructuras sem√°nticas complejas\\n\"\n",
    "    \n",
    "    # Top 3 caracter√≠sticas m√°s persistentes\n",
    "    pers_values = []\n",
    "    for birth, death in persistence_por_dim[dim]:\n",
    "        if death != float('inf'):\n",
    "            pers_values.append(death - birth)\n",
    "    \n",
    "    if pers_values:\n",
    "        top_pers = sorted(pers_values, reverse=True)[:3]\n",
    "        info_text += f\"  Top persistencias: {[f'{p:.3f}' for p in top_pers]}\\n\"\n",
    "    \n",
    "    info_text += \"\\n\"\n",
    "\n",
    "ax.text(0.05, 0.95, info_text, transform=ax.transAxes, \n",
    "        fontsize=10, verticalalignment='top', \n",
    "        fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo de barras\n",
    "fig, axes = plt.subplots(len(persistence_por_dim), 1, \n",
    "                         figsize=(14, 4 * len(persistence_por_dim)))\n",
    "\n",
    "if len(persistence_por_dim) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "colors_bar = ['red', 'blue', 'green', 'purple']\n",
    "\n",
    "for idx, dim in enumerate(sorted(persistence_por_dim.keys())):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Ordenar por nacimiento\n",
    "    intervals = sorted(persistence_por_dim[dim], key=lambda x: x[0])\n",
    "    \n",
    "    for i, (birth, death) in enumerate(intervals):\n",
    "        if death == float('inf'):\n",
    "            death = matriz_distancias[matriz_distancias < np.inf].max() * 1.1\n",
    "            ax.plot([birth, death], [i, i], color=colors_bar[dim % len(colors_bar)], \n",
    "                   linewidth=4, alpha=0.7, linestyle='--')\n",
    "        else:\n",
    "            ax.plot([birth, death], [i, i], color=colors_bar[dim % len(colors_bar)], \n",
    "                   linewidth=4, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Escala de Filtraci√≥n', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Caracter√≠stica', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'C√≥digo de Barras - H{dim}', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis Alternativo con Ripser\n",
    "\n",
    "Usamos Ripser como m√©todo complementario para validar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular persistencia con Ripser\n",
    "matriz_finita = matriz_distancias.copy()\n",
    "matriz_finita[np.isinf(matriz_finita)] = matriz_distancias[matriz_distancias < np.inf].max() * 2\n",
    "\n",
    "diagrams_ripser = ripser(matriz_finita, maxdim=2, distance_matrix=True)['dgms']\n",
    "\n",
    "print(\"Resultados con Ripser:\")\n",
    "for dim, dgm in enumerate(diagrams_ripser):\n",
    "    print(f\"  H{dim}: {len(dgm)} caracter√≠sticas\")\n",
    "\n",
    "# Visualizaci√≥n con persim\n",
    "fig, axes = plt.subplots(1, len(diagrams_ripser), figsize=(6*len(diagrams_ripser), 5))\n",
    "\n",
    "if len(diagrams_ripser) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for dim, (ax, dgm) in enumerate(zip(axes, diagrams_ripser)):\n",
    "    persim.plot_diagrams(dgm, ax=ax, legend=False)\n",
    "    ax.set_title(f'H{dim} - Diagrama de Persistencia (Ripser)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lisis e Interpretaci√≥n del Texto mediante ATD\n",
    "\n",
    "Interpretamos los resultados topol√≥gicos para caracterizar el texto:\n",
    "\n",
    "### Componentes Conexas (H‚ÇÄ)\n",
    "- Clusters de palabras representan temas o dominios sem√°nticos\n",
    "- M√∫ltiples componentes persistentes ‚Üí texto con m√∫ltiples temas\n",
    "- Componentes que nacen y mueren r√°pido ‚Üí transiciones narrativas\n",
    "\n",
    "### Ciclos (H‚ÇÅ)\n",
    "- Ciclos cortos ‚Üí relaciones bidireccionales (narrativa)\n",
    "- Ciclos largos ‚Üí conceptos interrelacionados (expositivo)\n",
    "- Muchos ciclos ‚Üí texto descriptivo con m√∫ltiples perspectivas\n",
    "\n",
    "### Cavidades (H‚ÇÇ)\n",
    "- Presencia de cavidades ‚Üí estructura argumentativa compleja\n",
    "- Ausencia de cavidades ‚Üí estructura lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_tipo_texto(persistence_por_dim, texto_original):\n",
    "    \"\"\"Analiza el tipo de texto bas√°ndose en caracter√≠sticas topol√≥gicas\"\"\"\n",
    "    \n",
    "    analisis = {\n",
    "        'narrativo': 0,\n",
    "        'descriptivo': 0,\n",
    "        'expositivo': 0,\n",
    "        'argumentativo': 0\n",
    "    }\n",
    "    \n",
    "    explicaciones = []\n",
    "    \n",
    "    # An√°lisis de H0 (componentes conexas)\n",
    "    if 0 in persistence_por_dim:\n",
    "        h0 = persistence_por_dim[0]\n",
    "        num_componentes = len(h0)\n",
    "        \n",
    "        # Persistencias de componentes\n",
    "        persistencias_h0 = []\n",
    "        for birth, death in h0:\n",
    "            if death != float('inf'):\n",
    "                persistencias_h0.append(death - birth)\n",
    "        \n",
    "        if persistencias_h0:\n",
    "            variabilidad = np.std(persistencias_h0)\n",
    "            \n",
    "            # Muchos componentes con alta variabilidad ‚Üí narrativo\n",
    "            if num_componentes > 5 and variabilidad > 0.05:\n",
    "                analisis['narrativo'] += 2\n",
    "                explicaciones.append(\n",
    "                    f\"‚úì NARRATIVO: {num_componentes} clusters con variabilidad {variabilidad:.3f} \"\n",
    "                    \"sugieren transiciones entre escenas/eventos\"\n",
    "                )\n",
    "            \n",
    "            # Pocos componentes muy persistentes ‚Üí descriptivo\n",
    "            if num_componentes <= 5 and max(persistencias_h0) > 0.2:\n",
    "                analisis['descriptivo'] += 2\n",
    "                explicaciones.append(\n",
    "                    f\"‚úì DESCRIPTIVO: Pocos clusters ({num_componentes}) muy persistentes \"\n",
    "                    \"indican enfoque en descripciones detalladas\"\n",
    "                )\n",
    "    \n",
    "    # An√°lisis de H1 (ciclos)\n",
    "    if 1 in persistence_por_dim:\n",
    "        h1 = persistence_por_dim[1]\n",
    "        num_ciclos = len(h1)\n",
    "        \n",
    "        persistencias_h1 = []\n",
    "        for birth, death in h1:\n",
    "            if death != float('inf'):\n",
    "                persistencias_h1.append(death - birth)\n",
    "        \n",
    "        if persistencias_h1:\n",
    "            pers_promedio = np.mean(persistencias_h1)\n",
    "            \n",
    "            # Muchos ciclos cortos ‚Üí narrativo (acciones que se entrelazan)\n",
    "            if num_ciclos > 3 and pers_promedio < 0.15:\n",
    "                analisis['narrativo'] += 1\n",
    "                explicaciones.append(\n",
    "                    f\"‚úì NARRATIVO: {num_ciclos} ciclos cortos (pers={pers_promedio:.3f}) \"\n",
    "                    \"representan relaciones causales entre eventos\"\n",
    "                )\n",
    "            \n",
    "            # Ciclos persistentes ‚Üí expositivo (conceptos interrelacionados)\n",
    "            if num_ciclos > 0 and pers_promedio > 0.15:\n",
    "                analisis['expositivo'] += 2\n",
    "                explicaciones.append(\n",
    "                    f\"‚úì EXPOSITIVO: Ciclos persistentes (pers={pers_promedio:.3f}) \"\n",
    "                    \"muestran interconexi√≥n conceptual\"\n",
    "                )\n",
    "            \n",
    "            # Muchos ciclos en general ‚Üí descriptivo\n",
    "            if num_ciclos > 5:\n",
    "                analisis['descriptivo'] += 1\n",
    "                explicaciones.append(\n",
    "                    f\"‚úì DESCRIPTIVO: {num_ciclos} ciclos indican m√∫ltiples relaciones \"\n",
    "                    \"entre elementos descritos\"\n",
    "                )\n",
    "    \n",
    "    # An√°lisis de H2 (cavidades)\n",
    "    if 2 in persistence_por_dim:\n",
    "        h2 = persistence_por_dim[2]\n",
    "        num_cavidades = len(h2)\n",
    "        \n",
    "        if num_cavidades > 0:\n",
    "            analisis['argumentativo'] += 2\n",
    "            analisis['expositivo'] += 1\n",
    "            explicaciones.append(\n",
    "                f\"‚úì ARGUMENTATIVO: {num_cavidades} cavidades 2D revelan estructura \"\n",
    "                \"argumentativa compleja o razonamiento elaborado\"\n",
    "            )\n",
    "        else:\n",
    "            explicaciones.append(\n",
    "                \"‚óã Sin cavidades 2D: estructura relativamente simple o lineal\"\n",
    "            )\n",
    "    \n",
    "    # An√°lisis textual complementario\n",
    "    oraciones = texto_original.split('.')\n",
    "    palabras_narrativas = ['comenz√≥', 'caminaba', 'escuch√≥', 'apareci√≥', 'continu√≥', \n",
    "                          'encontr√≥', 'lleg√≥', 'sac√≥']\n",
    "    palabras_descriptivas = ['era', 'denso', 'oscuro', 'altos', 'verde', 'cristalina', \n",
    "                            'azul', 'peque√±o']\n",
    "    palabras_expositivas = ['es', 'proceso', 'mediante', 'fundamental', 'mecanismo']\n",
    "    palabras_argumentativas = ['evidente', 'debemos', 'porque', 'necesidad', 'imperativa']\n",
    "    \n",
    "    texto_lower = texto_original.lower()\n",
    "    \n",
    "    count_narrativas = sum(1 for p in palabras_narrativas if p in texto_lower)\n",
    "    count_descriptivas = sum(1 for p in palabras_descriptivas if p in texto_lower)\n",
    "    count_expositivas = sum(1 for p in palabras_expositivas if p in texto_lower)\n",
    "    count_argumentativas = sum(1 for p in palabras_argumentativas if p in texto_lower)\n",
    "    \n",
    "    analisis['narrativo'] += count_narrativas * 0.5\n",
    "    analisis['descriptivo'] += count_descriptivas * 0.5\n",
    "    analisis['expositivo'] += count_expositivas * 0.5\n",
    "    analisis['argumentativo'] += count_argumentativas * 0.5\n",
    "    \n",
    "    return analisis, explicaciones\n",
    "\n",
    "analisis, explicaciones = analizar_tipo_texto(persistence_por_dim, texto_ejemplo)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS DEL TIPO DE TEXTO MEDIANTE ATD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä EVIDENCIAS TOPOL√ìGICAS:\\n\")\n",
    "for exp in explicaciones:\n",
    "    print(f\"  {exp}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà PUNTUACIONES POR TIPO DE TEXTO:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Normalizar puntuaciones\n",
    "total = sum(analisis.values())\n",
    "if total > 0:\n",
    "    for tipo in sorted(analisis.keys(), key=lambda x: analisis[x], reverse=True):\n",
    "        porcentaje = (analisis[tipo] / total) * 100\n",
    "        barra = '‚ñà' * int(porcentaje / 2)\n",
    "        print(f\"  {tipo.upper():15s}: {porcentaje:5.1f}% {barra}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ CONCLUSI√ìN:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tipo_dominante = max(analisis, key=analisis.get)\n",
    "segundo_tipo = sorted(analisis, key=analisis.get, reverse=True)[1]\n",
    "\n",
    "print(f\"\\nEl texto es principalmente {tipo_dominante.upper()} con elementos\")\n",
    "print(f\"{segundo_tipo.upper()}S.\")\n",
    "print(f\"\\nEsto se evidencia en la estructura topol√≥gica que revela:\")\n",
    "if tipo_dominante == 'narrativo':\n",
    "    print(\"  - M√∫ltiples transiciones temporales y cambios de escena\")\n",
    "    print(\"  - Relaciones causales entre eventos\")\n",
    "    print(\"  - Progresi√≥n temporal marcada\")\n",
    "elif tipo_dominante == 'descriptivo':\n",
    "    print(\"  - Clusters densos de vocabulario descriptivo\")\n",
    "    print(\"  - M√∫ltiples relaciones entre caracter√≠sticas de entidades\")\n",
    "    print(\"  - Enfoque en detalles y cualidades\")\n",
    "elif tipo_dominante == 'expositivo':\n",
    "    print(\"  - Conceptos fuertemente interconectados\")\n",
    "    print(\"  - Estructura explicativa clara\")\n",
    "    print(\"  - Relaciones conceptuales complejas\")\n",
    "elif tipo_dominante == 'argumentativo':\n",
    "    print(\"  - Estructura argumentativa de alto nivel\")\n",
    "    print(\"  - Razonamiento elaborado\")\n",
    "    print(\"  - Conexiones l√≥gicas complejas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n final: resumen del an√°lisis\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: Distribuci√≥n de tipos de texto\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "tipos = list(analisis.keys())\n",
    "valores = [analisis[t] for t in tipos]\n",
    "colores = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "bars = ax1.barh(tipos, valores, color=colores, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_xlabel('Puntuaci√≥n', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Caracterizaci√≥n del Texto por Tipo', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bar, val in zip(bars, valores):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.1f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Panel 2: Caracter√≠sticas topol√≥gicas por dimensi√≥n\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "dims = sorted(persistence_por_dim.keys())\n",
    "counts = [len(persistence_por_dim[d]) for d in dims]\n",
    "\n",
    "ax2.bar([f'H{d}' for d in dims], counts, color=['red', 'blue', 'green'][:len(dims)], \n",
    "        alpha=0.6, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('N√∫mero de Caracter√≠sticas', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Caracter√≠sticas Topol√≥gicas', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 3: Persistencia promedio por dimensi√≥n\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "pers_promedios = []\n",
    "\n",
    "for dim in dims:\n",
    "    pers = []\n",
    "    for birth, death in persistence_por_dim[dim]:\n",
    "        if death != float('inf'):\n",
    "            pers.append(death - birth)\n",
    "    if pers:\n",
    "        pers_promedios.append(np.mean(pers))\n",
    "    else:\n",
    "        pers_promedios.append(0)\n",
    "\n",
    "ax3.bar([f'H{d}' for d in dims], pers_promedios, \n",
    "        color=['red', 'blue', 'green'][:len(dims)], \n",
    "        alpha=0.6, edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('Persistencia Promedio', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Longevidad de Caracter√≠sticas', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 4: Resumen textual\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "resumen = f\"\"\"\n",
    "RESUMEN DEL AN√ÅLISIS TOPOL√ìGICO\n",
    "{'='*80}\n",
    "\n",
    "Texto analizado: {len(tokens)} tokens, {len(set(tokens))} palabras √∫nicas\n",
    "\n",
    "Caracter√≠sticas Topol√≥gicas Detectadas:\n",
    "  ‚Ä¢ H‚ÇÄ: {len(persistence_por_dim.get(0, []))} componentes conexas ‚Üí {len(persistence_por_dim.get(0, []))} clusters tem√°ticos\n",
    "  ‚Ä¢ H‚ÇÅ: {len(persistence_por_dim.get(1, []))} ciclos ‚Üí Relaciones sem√°nticas circulares\n",
    "  ‚Ä¢ H‚ÇÇ: {len(persistence_por_dim.get(2, []))} cavidades ‚Üí Estructuras de orden superior\n",
    "\n",
    "Tipo de Texto Dominante: {tipo_dominante.upper()} ({(analisis[tipo_dominante]/sum(analisis.values()))*100:.1f}%)\n",
    "\n",
    "Interpretaci√≥n:\n",
    "El an√°lisis topol√≥gico revela que el texto combina m√∫ltiples estilos discursivos.\n",
    "La presencia de {len(persistence_por_dim.get(0, []))} componentes conexas indica diferentes dominios sem√°nticos,\n",
    "mientras que los {len(persistence_por_dim.get(1, []))} ciclos detectados sugieren interconexiones conceptuales.\n",
    "\n",
    "Este patr√≥n es caracter√≠stico de textos que mezclan narraci√≥n con exposici√≥n y descripci√≥n,\n",
    "t√≠pico de textos educativos o divulgativos que utilizan ejemplos narrativos.\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, resumen, transform=ax4.transAxes,\n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.suptitle('An√°lisis Topol√≥gico de Datos - Resumen Completo', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusiones\n",
    "\n",
    "Este pipeline de ATD ha permitido:\n",
    "\n",
    "1. **Construir una representaci√≥n topol√≥gica** del texto mediante grafos de co-ocurrencia y complejos simpliciales\n",
    "\n",
    "2. **Identificar caracter√≠sticas persistentes** que revelan la estructura sem√°ntica subyacente\n",
    "\n",
    "3. **Caracterizar el tipo de texto** bas√°ndose en patrones topol√≥gicos:\n",
    "   - Componentes conexas revelan organizaci√≥n tem√°tica\n",
    "   - Ciclos indican relaciones conceptuales complejas\n",
    "   - Cavidades sugieren estructuras argumentativas elaboradas\n",
    "\n",
    "4. **Descubrir contenido mixto** donde el texto combina elementos narrativos, descriptivos, expositivos y argumentativos\n",
    "\n",
    "### Ventajas del ATD para an√°lisis de texto:\n",
    "- Captura relaciones de orden superior (m√°s all√° de pares)\n",
    "- Robusto ante ruido y variaciones\n",
    "- Revela estructura global y local simult√°neamente\n",
    "- Independiente de la longitud del texto\n",
    "\n",
    "### Aplicaciones futuras:\n",
    "- Clasificaci√≥n autom√°tica de g√©neros textuales\n",
    "- Detecci√≥n de cambios de estilo en textos largos\n",
    "- An√°lisis comparativo de autores\n",
    "- Identificaci√≥n de plagios estructurales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
